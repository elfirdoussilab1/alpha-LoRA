{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0944bc",
   "metadata": {},
   "source": [
    "----\n",
    "# BERT fine-tuning from scratch: Coding the Modifed LoRA and evaluating the model on Sentiment Analysis\n",
    "\n",
    "In this notebook, we will finetune a BERT model to perform sentiment analysis.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d06411",
   "metadata": {},
   "source": [
    "### Implementing our modified version of LoRA\n",
    "\n",
    "This is **done** in the file `models.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c720e6a",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2943ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aelfirdo/Desktop/Research/Transfer-Learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6e6d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee72ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = (\"test.csv\", \"train.csv\", \"val.csv\")\n",
    "download = True\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(os.path.join(\"data/sentiment\", f)):\n",
    "        download = False\n",
    "\n",
    "if download is False:\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    partition_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34087f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(\"data/sentiment\", \"train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(\"data/sentiment\", \"val.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"data/sentiment\", \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131b5593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i really did not watch this show as often when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>OK, I got the DVD set last week and I am final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>This is a bizarre oddity, directed by the guy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i liked this movie a lot.I rented this expecti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I never saw Doctor Who before (at least not in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  label\n",
       "0      0  i really did not watch this show as often when...      1\n",
       "1      0  OK, I got the DVD set last week and I am final...      1\n",
       "2      0  This is a bizarre oddity, directed by the guy ...      1\n",
       "3      0  i liked this movie a lot.I rented this expecti...      1\n",
       "4      0  I never saw Doctor Who before (at least not in...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db795864",
   "metadata": {},
   "source": [
    "### 2- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c25aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 35000 examples [00:00, 87679.97 examples/s]\n",
      "Generating validation split: 5000 examples [00:00, 90001.50 examples/s]\n",
      "Generating test split: 10000 examples [00:00, 91654.74 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 35000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": os.path.join(\"data/sentiment\", \"train.csv\"),\n",
    "        \"validation\": os.path.join(\"data/sentiment\", \"val.csv\"),\n",
    "        \"test\": os.path.join(\"data/sentiment\", \"test.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf06f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 50265\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b09195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch, truncation = True):\n",
    "    return tokenizer(batch[\"text\"], truncation=truncation, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "127222aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imdb_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m imdb_tokenized = \u001b[43mimdb_dataset\u001b[49m.map(tokenize_text, batched=\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'imdb_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519402e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7308d110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(imdb_tokenized['train']['input_ids'])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7f48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab03bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca26ecf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa3f2c",
   "metadata": {},
   "source": [
    "### 3- Set up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1953e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d73ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<processing.dataset_utils.IMDBDataset object at 0x38b27d650>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f64d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "def sample_n(dataset, n):\n",
    "    assert n <= len(dataset), \"Cannot sample more than the dataset size\"\n",
    "    indices = random.sample(range(len(dataset)), n)\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "N = 2000\n",
    "# Example usage\n",
    "sampled_dataset = sample_n(train_dataset, N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c9cdbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(sampled_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1bc54",
   "metadata": {},
   "source": [
    "### 4- Initializing BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478d28e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c236007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f840a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "# LoRA parameters\n",
    "rank = 8\n",
    "alpha = 0.5\n",
    "alpha_r = rank\n",
    "lora = False\n",
    "\n",
    "# Replacing Linear with LoRA\n",
    "make_adapter_roberta(model, lora, rank, alpha, alpha_r,device, train_alpha= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c69785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Adapter(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (adapter): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Adapter(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (adapter): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d77414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight: False\n",
      "roberta.embeddings.position_embeddings.weight: False\n",
      "roberta.embeddings.token_type_embeddings.weight: False\n",
      "roberta.embeddings.LayerNorm.weight: False\n",
      "roberta.embeddings.LayerNorm.bias: False\n",
      "roberta.encoder.layer.0.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.0.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.0.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.0.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.0.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.0.attention.self.key.weight: False\n",
      "roberta.encoder.layer.0.attention.self.key.bias: False\n",
      "roberta.encoder.layer.0.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.0.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.0.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.0.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.0.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.0.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.0.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.0.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.0.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.0.output.dense.weight: False\n",
      "roberta.encoder.layer.0.output.dense.bias: False\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.1.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.1.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.1.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.1.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.1.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.1.attention.self.key.weight: False\n",
      "roberta.encoder.layer.1.attention.self.key.bias: False\n",
      "roberta.encoder.layer.1.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.1.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.1.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.1.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.1.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.1.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.1.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.1.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.1.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.1.output.dense.weight: False\n",
      "roberta.encoder.layer.1.output.dense.bias: False\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.2.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.2.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.2.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.2.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.2.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.2.attention.self.key.weight: False\n",
      "roberta.encoder.layer.2.attention.self.key.bias: False\n",
      "roberta.encoder.layer.2.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.2.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.2.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.2.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.2.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.2.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.2.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.2.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.2.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.2.output.dense.weight: False\n",
      "roberta.encoder.layer.2.output.dense.bias: False\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.3.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.3.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.3.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.3.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.3.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.3.attention.self.key.weight: False\n",
      "roberta.encoder.layer.3.attention.self.key.bias: False\n",
      "roberta.encoder.layer.3.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.3.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.3.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.3.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.3.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.3.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.3.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.3.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.3.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.3.output.dense.weight: False\n",
      "roberta.encoder.layer.3.output.dense.bias: False\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.4.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.4.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.4.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.4.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.4.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.4.attention.self.key.weight: False\n",
      "roberta.encoder.layer.4.attention.self.key.bias: False\n",
      "roberta.encoder.layer.4.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.4.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.4.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.4.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.4.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.4.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.4.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.4.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.4.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.4.output.dense.weight: False\n",
      "roberta.encoder.layer.4.output.dense.bias: False\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.5.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.5.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.5.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.5.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.5.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.5.attention.self.key.weight: False\n",
      "roberta.encoder.layer.5.attention.self.key.bias: False\n",
      "roberta.encoder.layer.5.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.5.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.5.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.5.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.5.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.5.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.5.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.5.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.5.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.5.output.dense.weight: False\n",
      "roberta.encoder.layer.5.output.dense.bias: False\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.6.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.6.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.6.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.6.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.6.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.6.attention.self.key.weight: False\n",
      "roberta.encoder.layer.6.attention.self.key.bias: False\n",
      "roberta.encoder.layer.6.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.6.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.6.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.6.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.6.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.6.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.6.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.6.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.6.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.6.output.dense.weight: False\n",
      "roberta.encoder.layer.6.output.dense.bias: False\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.7.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.7.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.7.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.7.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.7.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.7.attention.self.key.weight: False\n",
      "roberta.encoder.layer.7.attention.self.key.bias: False\n",
      "roberta.encoder.layer.7.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.7.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.7.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.7.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.7.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.7.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.7.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.7.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.7.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.7.output.dense.weight: False\n",
      "roberta.encoder.layer.7.output.dense.bias: False\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.8.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.8.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.8.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.8.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.8.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.8.attention.self.key.weight: False\n",
      "roberta.encoder.layer.8.attention.self.key.bias: False\n",
      "roberta.encoder.layer.8.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.8.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.8.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.8.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.8.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.8.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.8.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.8.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.8.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.8.output.dense.weight: False\n",
      "roberta.encoder.layer.8.output.dense.bias: False\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.9.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.9.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.9.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.9.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.9.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.9.attention.self.key.weight: False\n",
      "roberta.encoder.layer.9.attention.self.key.bias: False\n",
      "roberta.encoder.layer.9.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.9.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.9.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.9.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.9.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.9.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.9.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.9.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.9.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.9.output.dense.weight: False\n",
      "roberta.encoder.layer.9.output.dense.bias: False\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.10.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.10.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.10.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.10.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.10.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.10.attention.self.key.weight: False\n",
      "roberta.encoder.layer.10.attention.self.key.bias: False\n",
      "roberta.encoder.layer.10.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.10.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.10.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.10.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.10.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.10.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.10.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.10.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.10.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.10.output.dense.weight: False\n",
      "roberta.encoder.layer.10.output.dense.bias: False\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.11.attention.self.query.alpha: False\n",
      "roberta.encoder.layer.11.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.11.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.11.attention.self.query.adapter.weight: True\n",
      "roberta.encoder.layer.11.attention.self.query.adapter.bias: True\n",
      "roberta.encoder.layer.11.attention.self.key.weight: False\n",
      "roberta.encoder.layer.11.attention.self.key.bias: False\n",
      "roberta.encoder.layer.11.attention.self.value.alpha: False\n",
      "roberta.encoder.layer.11.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.11.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.11.attention.self.value.adapter.weight: True\n",
      "roberta.encoder.layer.11.attention.self.value.adapter.bias: True\n",
      "roberta.encoder.layer.11.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.11.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.11.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.11.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.11.output.dense.weight: False\n",
      "roberta.encoder.layer.11.output.dense.bias: False\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias: False\n",
      "classifier.dense.weight: True\n",
      "classifier.dense.bias: True\n",
      "classifier.out_proj.weight: True\n",
      "classifier.out_proj.bias: True\n"
     ]
    }
   ],
   "source": [
    "# Check if LoRA was introduced correctly: Linear layers frozen and A, B trainable\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b299cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "lora_params, alpha_params = optimize_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8e2bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.5, dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(model.classifier.alpha.detach().cpu().numpy())\n",
    "model.roberta.encoder.layer[0].attention.self.query.alpha[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad79f6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the alpha\n",
    "change_alpha(model, 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdd528d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.699999988079071"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roberta.encoder.layer[1].attention.self.value.alpha[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff44c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters of the BERT model is : 138839810\n",
      "The number of trainable parameters after applying LoRA : 14766338\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "lora_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The total number of parameters of the BERT model is : {total_params}\")\n",
    "print(f\"The number of trainable parameters after applying LoRA : {lora_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa527e3",
   "metadata": {},
   "source": [
    "### 5- Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34a958d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = {'train': train_loader, 'val': val_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e55bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def evaluate_model(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['val', 'test']:\n",
    "        data_loader = loader[split]\n",
    "        acc = 0\n",
    "        loss = 0\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Perform a forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss += outputs.loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            acc += (predictions == labels).sum().item()\n",
    "        out[split + '_acc'] = acc / len(data_loader)\n",
    "        out[split + '_loss'] = loss / len(data_loader)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train(model, params):\n",
    "    # Set up the optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    n = len(train_loader)\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        # Training over this epoch\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        # Progress bar for the training phase\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{params['epochs']} [Training]\")\n",
    "\n",
    "        for i, batch in enumerate(train_progress_bar):\n",
    "\n",
    "            if i % params['inter_eval'] == 0:\n",
    "                # Evaluate the model\n",
    "                evals = evaluate_model(model)\n",
    "                print(f'Epoch {epoch} / {params[\"epochs\"]}, Step: {i} / {n} :')\n",
    "                print(f'Val Accuracy = {evals[\"val_acc\"]}, Val Loss = {evals[\"val_loss\"]},  Test Accuracy = {evals[\"test_acc\"]}, Test Loss = {evals[\"test_loss\"]}')\n",
    "                model.train()\n",
    "\n",
    "            # Move batch data to the correct device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Zero out the gradients from the previous iteration\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item() / n\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model's weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the progress bar\n",
    "            train_progress_bar.set_postfix({'training_loss': f'{loss.item():.3f}'})\n",
    "\n",
    "            # Update Train accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            train_acc += (predictions == labels).sum().item() / n\n",
    "        print(f'Finished Epoch {epoch} / {params[\"epochs\"]}: Train Loss = {train_loss}, Train Accuracy = {train_acc}')\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1810e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Do not launch this if you don't have GPU\n",
    "params = {'learning_rate': 1e-3,\n",
    "          'epochs': 100,\n",
    "          'inter_eval': 50}\n",
    "train(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ee008",
   "metadata": {},
   "source": [
    "## Trying to quantify the correlation between the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea994e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the embeddings from the BERT model\n",
    "def get_embedding_bert(sentence):\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        # The model_output is a tuple. The first element contains the token embeddings.\n",
    "        token_embeddings = model_output[0] # or model_output.last_hidden_state\n",
    "        \n",
    "        # Expand the attention mask to match the size of the token embeddings.\n",
    "        # This is needed to correctly mask the padding tokens.\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        \n",
    "        # Sum the embeddings, but only for the actual tokens (not padding).\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        \n",
    "        # Sum the attention mask to get the number of actual tokens.\n",
    "        # We clamp the sum to a minimum of 1e-9 to avoid division by zero.\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        # Divide the sum of embeddings by the number of tokens to get the mean.\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    # 1. Tokenize the input sentence.\n",
    "    #    - `return_tensors='pt'`: Return PyTorch tensors.\n",
    "    encoded_input = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # 2. Feed the tokenized input to the model.\n",
    "    #    - `torch.no_grad()` is used to disable gradient calculations, which saves memory\n",
    "    #      and speeds up computation, as we are not training the model.\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input) # model_output.last_hidden_state.shape = (1, sentence_men, embed_dim)\n",
    "        \n",
    "    # 3. Perform mean pooling on the token embeddings.\n",
    "    #    This will generate a single vector representation for the sentence.\n",
    "    sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # The result is a tensor of shape [1, embedding_dim]. We squeeze it to get a 1D tensor.\n",
    "    return sentence_embedding.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "topic_1 = 'Sentiment analysis'\n",
    "topic_2 = 'Safety alignment'\n",
    "\n",
    "# Tokenizing the sentences\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Get the embeddings\n",
    "mu_1 = get_embedding_bert(topic_1).detach().numpy()\n",
    "mu_2 = get_embedding_bert(topic_2).detach().numpy()\n",
    "\n",
    "# Making the vectors of norm 1\n",
    "mu_1 = mu_1 / np.linalg.norm(mu_1)\n",
    "mu_2 = mu_2 / np.linalg.norm(mu_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73a68334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8394884\n"
     ]
    }
   ],
   "source": [
    "# Getting the alignement of topic_2 with respect to topic_1\n",
    "beta = np.sum(mu_1 * mu_2)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c808d85",
   "metadata": {},
   "source": [
    "**Conclusion:** This approach is not performing super well with the distilbert model, as it gives a relatively high alignment score even for non-correlated tasks. Example, try Sentiment analysis with Image classification.\n",
    "\n",
    "We are trying now another BERT model that is specifically trained to produce *semantically meaningful embeddings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ef4f82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5297174\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "topic_1 = 'Cars classification'\n",
    "topic_2 = 'Animals classification'\n",
    "\n",
    "mu_1 = sentence_model.encode(topic_1)\n",
    "mu_2 = sentence_model.encode(topic_2)\n",
    "\n",
    "# Normalizing the vectors\n",
    "mu_1 = mu_1 / np.linalg.norm(mu_1)\n",
    "mu_2 = mu_2 / np.linalg.norm(mu_2)\n",
    "\n",
    "# Getting the alignement of topic_2 with respect to topic_1\n",
    "beta = np.sum(mu_1 * mu_2)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c18d92",
   "metadata": {},
   "source": [
    "**Remark:** This model seems to be okey. There is a logic in its results as it preserves the *ordering* of the alignment scores $\\beta$: lower scores to less correlated tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160937a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aelfirdo/Desktop/Research/Transfer-Learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading GLUE dataset for task: mnli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|| 392702/392702 [00:00<00:00, 2804028.49 examples/s]\n",
      "Generating validation_matched split: 100%|| 9815/9815 [00:00<00:00, 2307313.85 examples/s]\n",
      "Generating validation_mismatched split: 100%|| 9832/9832 [00:00<00:00, 2474699.77 examples/s]\n",
      "Generating test_matched split: 100%|| 9796/9796 [00:00<00:00, 682265.65 examples/s]\n",
      "Generating test_mismatched split: 100%|| 9847/9847 [00:00<00:00, 1465989.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "tain, val, test = get_glue_datasets('MNLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1261d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 392702\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1bb35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "    num_rows: 9815\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e828ab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNLI'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'Mnli'\n",
    "s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ace44c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aelfirdo/Desktop/Research/Transfer-Learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading GLUE dataset for task: mnli...\n",
      " Dataset loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 9815/9815 [00:00<00:00, 15722.72 examples/s]\n",
      "Map: 100%|| 9832/9832 [00:00<00:00, 19289.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from dataset import *\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_scheduler\n",
    "from model import *\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "import argparse\n",
    "from utils import fix_seed, evaluate_bert_accuracy\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "task_name = 'mnli'\n",
    "# Tokenizer and datasets\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_data, val_data, test_data = get_glue_datasets(task_name)\n",
    "# Define the sentence keys for each GLUE task. Most have two sentences.\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "sentence1_key, sentence2_key = task_to_keys[task_name.lower()]\n",
    "def preprocess_function(examples):\n",
    "    # RoBERTa tokenizer can handle one or two sentences.\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, padding = True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, padding = True)\n",
    "\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "\n",
    "# Remove original text columns and set format to PyTorch tensors\n",
    "tokenized_train = tokenized_train.remove_columns([k for k in task_to_keys[task_name.lower()] if k is not None] + ['idx'])\n",
    "tokenized_val = tokenized_val.remove_columns([k for k in task_to_keys[task_name.lower()] if k is not None] + ['idx'])\n",
    "tokenized_test = tokenized_test.remove_columns([k for k in task_to_keys[task_name.lower()] if k is not None] + ['idx'])\n",
    "tokenized_train.set_format(\"torch\")\n",
    "tokenized_val.set_format(\"torch\")\n",
    "tokenized_test.set_format(\"torch\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(tokenized_train, shuffle=True, batch_size=64)\n",
    "val_loader = DataLoader(tokenized_val, batch_size=64)\n",
    "test_loader = DataLoader(tokenized_test, batch_size=64)\n",
    "loader = {'train': train_loader, 'val': val_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4486165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 392702\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8471141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 0, 2, 2, 2, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_val['label'][0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e27c240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_test['label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b9eb9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    0,   725,   906, 15609,     6, 33141, 15636,   139,     6, 33141,\n",
       "           385,   922,  1755,     6,     8, 25805,  3019,    32,    95,    10,\n",
       "           367,  2523,   966,  2396,    10,   356,    12,   995,    13,     4,\n",
       "             2,     2,   725,   906, 15609,    16,    10,   766,   966,   546,\n",
       "            66,    13,     4,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " tensor([    0,   133,  5239,     9,     5, 19184,  3038,    74,  6723,    11,\n",
       "           233,    15,     5,  3184,     9,     5,  1736,  1316,   586,     8,\n",
       "           143,  4971,    15, 13655,     5,  1188,     4,     2,     2, 10787,\n",
       "            82,    74,    28,   182, 13865,     7,  7082,   797,    81,    49,\n",
       "           308,   418,     4,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " tensor([    0, 14699,   352,   899,     7,   335,    16,    11,     5,   275,\n",
       "          3168,     9,   258,  9575,   673,     8,     5,  2244,     4,     2,\n",
       "             2,   243,    16,    11,   961,    18,   275,   773,     7,    33,\n",
       "           899,     7,   335,    11,    10, 10358,  4737,     4,     2,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " tensor([    0, 20930,    11,     5, 14870,  2802, 16993,   415, 18179,  1139,\n",
       "             9,   468,  1725,   219,     6,     5,  1515,   168,   747,  4362,\n",
       "            55,   992, 30783,    87,    63, 22337,    11, 38919,  2366, 26203,\n",
       "             8,  5523,    62,  1475,    12, 34696,  2309,     4,     2,     2,\n",
       "           133,  1515,   168,  1595,  1475,    12, 34696,  2074,  3448,    23,\n",
       "          1903,     5, 12281,     4,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " tensor([    0, 46923,    11, 41102,     6,    63, 34365,     9, 31789,  4049,\n",
       "             8,  2471,  6440,    16,     5,  7763,    11,  7378,   131,    63,\n",
       "         15947,  6291, 37364,    16,    67,  6097,     9,     5,  3567,     4,\n",
       "             2,     2,   243,    21, 11236,    11, 41102,     8,    34,     5,\n",
       "          7763, 34365,    11,  7378,     4,     2,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " tensor([    0,  4015,    10,    76,     8,    47,   120,    10,   481,  4085,\n",
       "             8, 38596,     6, 38596,     6, 38596,     4,     2,     2,   133,\n",
       "           701,    21,  6164,    10,    76,     8,  1165,    10,  4085,     4,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " tensor([    0, 33173, 14102,    58,   998, 14573,  7752,     7, 10507,  7656,\n",
       "          5812,    11,   986,   107,     4,     2,     2, 10787,  1252,   218,\n",
       "            75,   240,  7656,     4,     2,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " tensor([    0, 11195,    54,   109,    45,  1871,     8,    54,   109,    45,\n",
       "            33, 15131,    40,    33,     7,  6723,  2743,    15,  3574,  2010,\n",
       "            11,    49,   793,  1046,     4,     2,     2, 11195,    54,   109,\n",
       "            45,  1871,     8,    54,   109,    45,    33, 15131,   218,    75,\n",
       "           240,  3574,  2010,   323,    23,    70,    77,    51,   214,  7497,\n",
       "             4,  1437,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " tensor([    0,   405,    18,    41, 17984,     8,     8,    24,    18,     8,\n",
       "            24,    18,   716,   716,    15,    15, 37463, 10518,    14,   951,\n",
       "            34,     9,   141,    10,  1262,  3018,    74, 18871,    77,   103,\n",
       "          2196,  1434, 18871,     2,     2,  1213,    32, 29744,    95,   101,\n",
       "            10,  3018,     6,    51,   531,    28,    65,     4,  1437,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " tensor([    0, 10462,     6,    65,    64,  6876,    14,   103,  1484,  4997,\n",
       "             7,  1416,    56,   182,  1313,  7762,    11,  1110,     9, 14138,\n",
       "            11,  4835,     8,     9,   810, 11729,     4,     2,     2,   243,\n",
       "            16,   678,   103,  1484,    56,   182,  1313,  7762,     4,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test['input_ids'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67db1d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2-1.5B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_name = 'Qwen/Qwen2-1.5B'\n",
    "num_classes = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=num_classes\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44a78df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForSequenceClassification(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=1536, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae51dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "model_name = 'roberta-base'\n",
    "num_classes = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, \n",
    "        num_labels=num_classes\n",
    "    )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4852dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = getattr(model, 'classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621cb0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaClassificationHead(\n",
      "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2748677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma3TextModel were not initialized from the model checkpoint at google/gemma-3-270m and are newly initialized: ['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.mlp.down_proj.weight', 'layers.0.mlp.gate_proj.weight', 'layers.0.mlp.up_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.post_feedforward_layernorm.weight', 'layers.0.pre_feedforward_layernorm.weight', 'layers.0.self_attn.k_norm.weight', 'layers.0.self_attn.k_proj.weight', 'layers.0.self_attn.o_proj.weight', 'layers.0.self_attn.q_norm.weight', 'layers.0.self_attn.q_proj.weight', 'layers.0.self_attn.v_proj.weight', 'layers.1.input_layernorm.weight', 'layers.1.mlp.down_proj.weight', 'layers.1.mlp.gate_proj.weight', 'layers.1.mlp.up_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.post_feedforward_layernorm.weight', 'layers.1.pre_feedforward_layernorm.weight', 'layers.1.self_attn.k_norm.weight', 'layers.1.self_attn.k_proj.weight', 'layers.1.self_attn.o_proj.weight', 'layers.1.self_attn.q_norm.weight', 'layers.1.self_attn.q_proj.weight', 'layers.1.self_attn.v_proj.weight', 'layers.10.input_layernorm.weight', 'layers.10.mlp.down_proj.weight', 'layers.10.mlp.gate_proj.weight', 'layers.10.mlp.up_proj.weight', 'layers.10.post_attention_layernorm.weight', 'layers.10.post_feedforward_layernorm.weight', 'layers.10.pre_feedforward_layernorm.weight', 'layers.10.self_attn.k_norm.weight', 'layers.10.self_attn.k_proj.weight', 'layers.10.self_attn.o_proj.weight', 'layers.10.self_attn.q_norm.weight', 'layers.10.self_attn.q_proj.weight', 'layers.10.self_attn.v_proj.weight', 'layers.11.input_layernorm.weight', 'layers.11.mlp.down_proj.weight', 'layers.11.mlp.gate_proj.weight', 'layers.11.mlp.up_proj.weight', 'layers.11.post_attention_layernorm.weight', 'layers.11.post_feedforward_layernorm.weight', 'layers.11.pre_feedforward_layernorm.weight', 'layers.11.self_attn.k_norm.weight', 'layers.11.self_attn.k_proj.weight', 'layers.11.self_attn.o_proj.weight', 'layers.11.self_attn.q_norm.weight', 'layers.11.self_attn.q_proj.weight', 'layers.11.self_attn.v_proj.weight', 'layers.12.input_layernorm.weight', 'layers.12.mlp.down_proj.weight', 'layers.12.mlp.gate_proj.weight', 'layers.12.mlp.up_proj.weight', 'layers.12.post_attention_layernorm.weight', 'layers.12.post_feedforward_layernorm.weight', 'layers.12.pre_feedforward_layernorm.weight', 'layers.12.self_attn.k_norm.weight', 'layers.12.self_attn.k_proj.weight', 'layers.12.self_attn.o_proj.weight', 'layers.12.self_attn.q_norm.weight', 'layers.12.self_attn.q_proj.weight', 'layers.12.self_attn.v_proj.weight', 'layers.13.input_layernorm.weight', 'layers.13.mlp.down_proj.weight', 'layers.13.mlp.gate_proj.weight', 'layers.13.mlp.up_proj.weight', 'layers.13.post_attention_layernorm.weight', 'layers.13.post_feedforward_layernorm.weight', 'layers.13.pre_feedforward_layernorm.weight', 'layers.13.self_attn.k_norm.weight', 'layers.13.self_attn.k_proj.weight', 'layers.13.self_attn.o_proj.weight', 'layers.13.self_attn.q_norm.weight', 'layers.13.self_attn.q_proj.weight', 'layers.13.self_attn.v_proj.weight', 'layers.14.input_layernorm.weight', 'layers.14.mlp.down_proj.weight', 'layers.14.mlp.gate_proj.weight', 'layers.14.mlp.up_proj.weight', 'layers.14.post_attention_layernorm.weight', 'layers.14.post_feedforward_layernorm.weight', 'layers.14.pre_feedforward_layernorm.weight', 'layers.14.self_attn.k_norm.weight', 'layers.14.self_attn.k_proj.weight', 'layers.14.self_attn.o_proj.weight', 'layers.14.self_attn.q_norm.weight', 'layers.14.self_attn.q_proj.weight', 'layers.14.self_attn.v_proj.weight', 'layers.15.input_layernorm.weight', 'layers.15.mlp.down_proj.weight', 'layers.15.mlp.gate_proj.weight', 'layers.15.mlp.up_proj.weight', 'layers.15.post_attention_layernorm.weight', 'layers.15.post_feedforward_layernorm.weight', 'layers.15.pre_feedforward_layernorm.weight', 'layers.15.self_attn.k_norm.weight', 'layers.15.self_attn.k_proj.weight', 'layers.15.self_attn.o_proj.weight', 'layers.15.self_attn.q_norm.weight', 'layers.15.self_attn.q_proj.weight', 'layers.15.self_attn.v_proj.weight', 'layers.16.input_layernorm.weight', 'layers.16.mlp.down_proj.weight', 'layers.16.mlp.gate_proj.weight', 'layers.16.mlp.up_proj.weight', 'layers.16.post_attention_layernorm.weight', 'layers.16.post_feedforward_layernorm.weight', 'layers.16.pre_feedforward_layernorm.weight', 'layers.16.self_attn.k_norm.weight', 'layers.16.self_attn.k_proj.weight', 'layers.16.self_attn.o_proj.weight', 'layers.16.self_attn.q_norm.weight', 'layers.16.self_attn.q_proj.weight', 'layers.16.self_attn.v_proj.weight', 'layers.17.input_layernorm.weight', 'layers.17.mlp.down_proj.weight', 'layers.17.mlp.gate_proj.weight', 'layers.17.mlp.up_proj.weight', 'layers.17.post_attention_layernorm.weight', 'layers.17.post_feedforward_layernorm.weight', 'layers.17.pre_feedforward_layernorm.weight', 'layers.17.self_attn.k_norm.weight', 'layers.17.self_attn.k_proj.weight', 'layers.17.self_attn.o_proj.weight', 'layers.17.self_attn.q_norm.weight', 'layers.17.self_attn.q_proj.weight', 'layers.17.self_attn.v_proj.weight', 'layers.2.input_layernorm.weight', 'layers.2.mlp.down_proj.weight', 'layers.2.mlp.gate_proj.weight', 'layers.2.mlp.up_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.post_feedforward_layernorm.weight', 'layers.2.pre_feedforward_layernorm.weight', 'layers.2.self_attn.k_norm.weight', 'layers.2.self_attn.k_proj.weight', 'layers.2.self_attn.o_proj.weight', 'layers.2.self_attn.q_norm.weight', 'layers.2.self_attn.q_proj.weight', 'layers.2.self_attn.v_proj.weight', 'layers.3.input_layernorm.weight', 'layers.3.mlp.down_proj.weight', 'layers.3.mlp.gate_proj.weight', 'layers.3.mlp.up_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.post_feedforward_layernorm.weight', 'layers.3.pre_feedforward_layernorm.weight', 'layers.3.self_attn.k_norm.weight', 'layers.3.self_attn.k_proj.weight', 'layers.3.self_attn.o_proj.weight', 'layers.3.self_attn.q_norm.weight', 'layers.3.self_attn.q_proj.weight', 'layers.3.self_attn.v_proj.weight', 'layers.4.input_layernorm.weight', 'layers.4.mlp.down_proj.weight', 'layers.4.mlp.gate_proj.weight', 'layers.4.mlp.up_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.post_feedforward_layernorm.weight', 'layers.4.pre_feedforward_layernorm.weight', 'layers.4.self_attn.k_norm.weight', 'layers.4.self_attn.k_proj.weight', 'layers.4.self_attn.o_proj.weight', 'layers.4.self_attn.q_norm.weight', 'layers.4.self_attn.q_proj.weight', 'layers.4.self_attn.v_proj.weight', 'layers.5.input_layernorm.weight', 'layers.5.mlp.down_proj.weight', 'layers.5.mlp.gate_proj.weight', 'layers.5.mlp.up_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.post_feedforward_layernorm.weight', 'layers.5.pre_feedforward_layernorm.weight', 'layers.5.self_attn.k_norm.weight', 'layers.5.self_attn.k_proj.weight', 'layers.5.self_attn.o_proj.weight', 'layers.5.self_attn.q_norm.weight', 'layers.5.self_attn.q_proj.weight', 'layers.5.self_attn.v_proj.weight', 'layers.6.input_layernorm.weight', 'layers.6.mlp.down_proj.weight', 'layers.6.mlp.gate_proj.weight', 'layers.6.mlp.up_proj.weight', 'layers.6.post_attention_layernorm.weight', 'layers.6.post_feedforward_layernorm.weight', 'layers.6.pre_feedforward_layernorm.weight', 'layers.6.self_attn.k_norm.weight', 'layers.6.self_attn.k_proj.weight', 'layers.6.self_attn.o_proj.weight', 'layers.6.self_attn.q_norm.weight', 'layers.6.self_attn.q_proj.weight', 'layers.6.self_attn.v_proj.weight', 'layers.7.input_layernorm.weight', 'layers.7.mlp.down_proj.weight', 'layers.7.mlp.gate_proj.weight', 'layers.7.mlp.up_proj.weight', 'layers.7.post_attention_layernorm.weight', 'layers.7.post_feedforward_layernorm.weight', 'layers.7.pre_feedforward_layernorm.weight', 'layers.7.self_attn.k_norm.weight', 'layers.7.self_attn.k_proj.weight', 'layers.7.self_attn.o_proj.weight', 'layers.7.self_attn.q_norm.weight', 'layers.7.self_attn.q_proj.weight', 'layers.7.self_attn.v_proj.weight', 'layers.8.input_layernorm.weight', 'layers.8.mlp.down_proj.weight', 'layers.8.mlp.gate_proj.weight', 'layers.8.mlp.up_proj.weight', 'layers.8.post_attention_layernorm.weight', 'layers.8.post_feedforward_layernorm.weight', 'layers.8.pre_feedforward_layernorm.weight', 'layers.8.self_attn.k_norm.weight', 'layers.8.self_attn.k_proj.weight', 'layers.8.self_attn.o_proj.weight', 'layers.8.self_attn.q_norm.weight', 'layers.8.self_attn.q_proj.weight', 'layers.8.self_attn.v_proj.weight', 'layers.9.input_layernorm.weight', 'layers.9.mlp.down_proj.weight', 'layers.9.mlp.gate_proj.weight', 'layers.9.mlp.up_proj.weight', 'layers.9.post_attention_layernorm.weight', 'layers.9.post_feedforward_layernorm.weight', 'layers.9.pre_feedforward_layernorm.weight', 'layers.9.self_attn.k_norm.weight', 'layers.9.self_attn.k_proj.weight', 'layers.9.self_attn.o_proj.weight', 'layers.9.self_attn.q_norm.weight', 'layers.9.self_attn.q_proj.weight', 'layers.9.self_attn.v_proj.weight', 'norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3TextModel(\n",
      "  (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-17): 18 x Gemma3DecoderLayer(\n",
      "      (self_attn): Gemma3Attention(\n",
      "        (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
      "        (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
      "        (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
      "        (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
      "        (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "        (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "      )\n",
      "      (mlp): Gemma3MLP(\n",
      "        (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
      "        (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
      "        (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
      "        (act_fn): PytorchGELUTanh()\n",
      "      )\n",
      "      (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "      (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "      (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "      (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "    )\n",
      "  )\n",
      "  (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
      "  (rotary_emb): Gemma3RotaryEmbedding()\n",
      "  (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "login(token= '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')\n",
    "#model_name = 'Qwen/Qwen2.5-0.5B'\n",
    "model_name = 'google/gemma-3-270m'\n",
    "num_classes = 3\n",
    "model = AutoModel.from_pretrained(\n",
    "        model_name, \n",
    "        #num_labels=num_classes\n",
    "    )\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea73a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "apply_adapter(model, model_name, lora = True, rank = 8, alpha = 1, alpha_r = 8, device = 'cpu', train_alpha = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b969046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Adapter(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Adapter(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7312cb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124961283"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = getattr(model, 'classifier')\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9796429",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
