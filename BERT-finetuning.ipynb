{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de0944bc",
   "metadata": {},
   "source": [
    "----\n",
    "# BERT fine-tuning from scratch: Coding the Modifed LoRA and evaluating the model on Sentiment Analysis\n",
    "\n",
    "In this notebook, we will finetune a BERT model to perform sentiment analysis.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d06411",
   "metadata": {},
   "source": [
    "### Implementing our modified version of LoRA\n",
    "\n",
    "This is **done** in the file `models.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c720e6a",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2943ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aelfirdo/Desktop/Research/Transfer-Learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from processing.dataset_utils import download_dataset, load_dataset_into_to_dataframe, partition_dataset, IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6e6d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee72ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = (\"test.csv\", \"train.csv\", \"val.csv\")\n",
    "download = True\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(os.path.join(\"data/sentiment\", f)):\n",
    "        download = False\n",
    "\n",
    "if download is False:\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    partition_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34087f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(\"data/sentiment\", \"train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(\"data/sentiment\", \"val.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"data/sentiment\", \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131b5593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i really did not watch this show as often when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>OK, I got the DVD set last week and I am final...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>This is a bizarre oddity, directed by the guy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>i liked this movie a lot.I rented this expecti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I never saw Doctor Who before (at least not in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  label\n",
       "0      0  i really did not watch this show as often when...      1\n",
       "1      0  OK, I got the DVD set last week and I am final...      1\n",
       "2      0  This is a bizarre oddity, directed by the guy ...      1\n",
       "3      0  i liked this movie a lot.I rented this expecti...      1\n",
       "4      0  I never saw Doctor Who before (at least not in...      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db795864",
   "metadata": {},
   "source": [
    "### 2- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c25aa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 35000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": os.path.join(\"data/sentiment\", \"train.csv\"),\n",
    "        \"validation\": os.path.join(\"data/sentiment\", \"val.csv\"),\n",
    "        \"test\": os.path.join(\"data/sentiment\", \"test.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf06f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01b09195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch, truncation = True):\n",
    "    return tokenizer(batch[\"text\"], truncation=truncation, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "127222aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35000/35000 [00:08<00:00, 3947.60 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4747.43 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:02<00:00, 3674.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "519402e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7308d110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35000, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(imdb_tokenized['train']['input_ids'])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7f48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ab03bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca26ecf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baa3f2c",
   "metadata": {},
   "source": [
    "### 3- Set up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1953e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1bc54",
   "metadata": {},
   "source": [
    "### 4- Initializing BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478d28e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c236007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f840a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "# LoRA parameters\n",
    "rank = 8\n",
    "alpha = 0.5\n",
    "alpha_r = rank\n",
    "\n",
    "# Replacing Linear with LoRA\n",
    "replace_lora_roberta(model, rank, alpha, alpha_r,device, train_alpha= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88c69785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSdpaSelfAttention(\n",
      "              (query): LoRALinear(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): LoRALinear(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d77414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.embeddings.word_embeddings.weight: False\n",
      "roberta.embeddings.position_embeddings.weight: False\n",
      "roberta.embeddings.token_type_embeddings.weight: False\n",
      "roberta.embeddings.LayerNorm.weight: False\n",
      "roberta.embeddings.LayerNorm.bias: False\n",
      "roberta.encoder.layer.0.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.0.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.0.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.0.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.0.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.0.attention.self.key.weight: False\n",
      "roberta.encoder.layer.0.attention.self.key.bias: False\n",
      "roberta.encoder.layer.0.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.0.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.0.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.0.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.0.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.0.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.0.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.0.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.0.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.0.output.dense.weight: False\n",
      "roberta.encoder.layer.0.output.dense.bias: False\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.1.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.1.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.1.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.1.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.1.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.1.attention.self.key.weight: False\n",
      "roberta.encoder.layer.1.attention.self.key.bias: False\n",
      "roberta.encoder.layer.1.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.1.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.1.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.1.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.1.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.1.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.1.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.1.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.1.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.1.output.dense.weight: False\n",
      "roberta.encoder.layer.1.output.dense.bias: False\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.2.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.2.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.2.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.2.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.2.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.2.attention.self.key.weight: False\n",
      "roberta.encoder.layer.2.attention.self.key.bias: False\n",
      "roberta.encoder.layer.2.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.2.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.2.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.2.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.2.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.2.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.2.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.2.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.2.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.2.output.dense.weight: False\n",
      "roberta.encoder.layer.2.output.dense.bias: False\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.3.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.3.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.3.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.3.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.3.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.3.attention.self.key.weight: False\n",
      "roberta.encoder.layer.3.attention.self.key.bias: False\n",
      "roberta.encoder.layer.3.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.3.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.3.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.3.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.3.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.3.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.3.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.3.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.3.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.3.output.dense.weight: False\n",
      "roberta.encoder.layer.3.output.dense.bias: False\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.4.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.4.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.4.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.4.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.4.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.4.attention.self.key.weight: False\n",
      "roberta.encoder.layer.4.attention.self.key.bias: False\n",
      "roberta.encoder.layer.4.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.4.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.4.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.4.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.4.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.4.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.4.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.4.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.4.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.4.output.dense.weight: False\n",
      "roberta.encoder.layer.4.output.dense.bias: False\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.5.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.5.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.5.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.5.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.5.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.5.attention.self.key.weight: False\n",
      "roberta.encoder.layer.5.attention.self.key.bias: False\n",
      "roberta.encoder.layer.5.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.5.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.5.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.5.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.5.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.5.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.5.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.5.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.5.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.5.output.dense.weight: False\n",
      "roberta.encoder.layer.5.output.dense.bias: False\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.6.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.6.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.6.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.6.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.6.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.6.attention.self.key.weight: False\n",
      "roberta.encoder.layer.6.attention.self.key.bias: False\n",
      "roberta.encoder.layer.6.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.6.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.6.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.6.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.6.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.6.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.6.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.6.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.6.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.6.output.dense.weight: False\n",
      "roberta.encoder.layer.6.output.dense.bias: False\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.7.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.7.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.7.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.7.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.7.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.7.attention.self.key.weight: False\n",
      "roberta.encoder.layer.7.attention.self.key.bias: False\n",
      "roberta.encoder.layer.7.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.7.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.7.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.7.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.7.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.7.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.7.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.7.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.7.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.7.output.dense.weight: False\n",
      "roberta.encoder.layer.7.output.dense.bias: False\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.8.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.8.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.8.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.8.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.8.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.8.attention.self.key.weight: False\n",
      "roberta.encoder.layer.8.attention.self.key.bias: False\n",
      "roberta.encoder.layer.8.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.8.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.8.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.8.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.8.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.8.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.8.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.8.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.8.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.8.output.dense.weight: False\n",
      "roberta.encoder.layer.8.output.dense.bias: False\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.9.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.9.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.9.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.9.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.9.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.9.attention.self.key.weight: False\n",
      "roberta.encoder.layer.9.attention.self.key.bias: False\n",
      "roberta.encoder.layer.9.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.9.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.9.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.9.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.9.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.9.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.9.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.9.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.9.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.9.output.dense.weight: False\n",
      "roberta.encoder.layer.9.output.dense.bias: False\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.10.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.10.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.10.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.10.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.10.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.10.attention.self.key.weight: False\n",
      "roberta.encoder.layer.10.attention.self.key.bias: False\n",
      "roberta.encoder.layer.10.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.10.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.10.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.10.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.10.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.10.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.10.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.10.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.10.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.10.output.dense.weight: False\n",
      "roberta.encoder.layer.10.output.dense.bias: False\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.11.attention.self.query.lora_A: True\n",
      "roberta.encoder.layer.11.attention.self.query.lora_B: True\n",
      "roberta.encoder.layer.11.attention.self.query.alpha: True\n",
      "roberta.encoder.layer.11.attention.self.query.linear.weight: False\n",
      "roberta.encoder.layer.11.attention.self.query.linear.bias: False\n",
      "roberta.encoder.layer.11.attention.self.key.weight: False\n",
      "roberta.encoder.layer.11.attention.self.key.bias: False\n",
      "roberta.encoder.layer.11.attention.self.value.lora_A: True\n",
      "roberta.encoder.layer.11.attention.self.value.lora_B: True\n",
      "roberta.encoder.layer.11.attention.self.value.alpha: True\n",
      "roberta.encoder.layer.11.attention.self.value.linear.weight: False\n",
      "roberta.encoder.layer.11.attention.self.value.linear.bias: False\n",
      "roberta.encoder.layer.11.attention.output.dense.weight: False\n",
      "roberta.encoder.layer.11.attention.output.dense.bias: False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias: False\n",
      "roberta.encoder.layer.11.intermediate.dense.weight: False\n",
      "roberta.encoder.layer.11.intermediate.dense.bias: False\n",
      "roberta.encoder.layer.11.output.dense.weight: False\n",
      "roberta.encoder.layer.11.output.dense.bias: False\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight: False\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias: False\n",
      "classifier.dense.weight: True\n",
      "classifier.dense.bias: True\n",
      "classifier.out_proj.weight: True\n",
      "classifier.out_proj.bias: True\n"
     ]
    }
   ],
   "source": [
    "# Check if LoRA was introduced correctly: Linear layers frozen and A, B trainable\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b299cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "lora_params, alpha_params = optimize_lora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef8e2bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(model.classifier.alpha.detach().cpu().numpy())\n",
    "model.roberta.encoder.layer[0].attention.self.query.alpha.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff44c040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of parameters of the BERT model is : 67637010\n",
      "The number of trainable parameters after applying LoRA : 682000\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "lora_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"The total number of parameters of the BERT model is : {total_params}\")\n",
    "print(f\"The number of trainable parameters after applying LoRA : {lora_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa527e3",
   "metadata": {},
   "source": [
    "### 5- Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34a958d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = {'train': train_loader, 'val': val_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e55bc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def evaluate_model(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split in ['val', 'test']:\n",
    "        data_loader = loader[split]\n",
    "        acc = 0\n",
    "        loss = 0\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Perform a forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss += outputs.loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            acc += (predictions == labels).sum().item()\n",
    "        out[split + '_acc'] = acc / len(data_loader)\n",
    "        out[split + '_loss'] = loss / len(data_loader)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c7afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def train(model, params):\n",
    "    # Set up the optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # Move the model to the specified device (GPU or CPU)\n",
    "    model.to(device)\n",
    "    n = len(train_loader)\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        # Training over this epoch\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        # Progress bar for the training phase\n",
    "        train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{params['epochs']} [Training]\")\n",
    "\n",
    "        for i, batch in enumerate(train_progress_bar):\n",
    "\n",
    "            if i % params['inter_eval'] == 0:\n",
    "                # Evaluate the model\n",
    "                evals = evaluate_model(model)\n",
    "                print(f'Epoch {epoch} / {params[\"epochs\"]}, Step: {i} / {n} :')\n",
    "                print(f'Val Accuracy = {evals[\"val_acc\"]}, Val Loss = {evals[\"val_loss\"]},  Test Accuracy = {evals[\"test_acc\"]}, Test Loss = {evals[\"test_loss\"]}')\n",
    "                model.train()\n",
    "\n",
    "            # Move batch data to the correct device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Zero out the gradients from the previous iteration\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = outputs.loss\n",
    "            train_loss += loss.item() / n\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the model's weights\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the progress bar\n",
    "            train_progress_bar.set_postfix({'training_loss': f'{loss.item():.3f}'})\n",
    "\n",
    "            # Update Train accuracy\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            train_acc += (predictions == labels).sum().item() / n\n",
    "        print(f'Finished Epoch {epoch} / {params[\"epochs\"]}: Train Loss = {train_loss}, Train Accuracy = {train_acc}')\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1810e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Do not launch this if you don't have GPU\n",
    "params = {'learning_rate': 1e-3,\n",
    "          'epochs': 100,\n",
    "          'inter_eval': 50}\n",
    "train(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ee008",
   "metadata": {},
   "source": [
    "## Trying to quantify the correlation between the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea994e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the embeddings from the BERT model\n",
    "def get_embedding_bert(sentence):\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        # The model_output is a tuple. The first element contains the token embeddings.\n",
    "        token_embeddings = model_output[0] # or model_output.last_hidden_state\n",
    "        \n",
    "        # Expand the attention mask to match the size of the token embeddings.\n",
    "        # This is needed to correctly mask the padding tokens.\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        \n",
    "        # Sum the embeddings, but only for the actual tokens (not padding).\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        \n",
    "        # Sum the attention mask to get the number of actual tokens.\n",
    "        # We clamp the sum to a minimum of 1e-9 to avoid division by zero.\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "        # Divide the sum of embeddings by the number of tokens to get the mean.\n",
    "        return sum_embeddings / sum_mask\n",
    "    \n",
    "    # 1. Tokenize the input sentence.\n",
    "    #    - `return_tensors='pt'`: Return PyTorch tensors.\n",
    "    encoded_input = tokenizer(sentence, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # 2. Feed the tokenized input to the model.\n",
    "    #    - `torch.no_grad()` is used to disable gradient calculations, which saves memory\n",
    "    #      and speeds up computation, as we are not training the model.\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input) # model_output.last_hidden_state.shape = (1, sentence_men, embed_dim)\n",
    "        \n",
    "    # 3. Perform mean pooling on the token embeddings.\n",
    "    #    This will generate a single vector representation for the sentence.\n",
    "    sentence_embedding = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # The result is a tensor of shape [1, embedding_dim]. We squeeze it to get a 1D tensor.\n",
    "    return sentence_embedding.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "619d680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "\n",
    "topic_1 = 'Sentiment analysis'\n",
    "topic_2 = 'Safety alignment'\n",
    "\n",
    "# Tokenizing the sentences\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Get the embeddings\n",
    "mu_1 = get_embedding_bert(topic_1).detach().numpy()\n",
    "mu_2 = get_embedding_bert(topic_2).detach().numpy()\n",
    "\n",
    "# Making the vectors of norm 1\n",
    "mu_1 = mu_1 / np.linalg.norm(mu_1)\n",
    "mu_2 = mu_2 / np.linalg.norm(mu_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73a68334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8394884\n"
     ]
    }
   ],
   "source": [
    "# Getting the alignement of topic_2 with respect to topic_1\n",
    "beta = np.sum(mu_1 * mu_2)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c808d85",
   "metadata": {},
   "source": [
    "**Conclusion:** This approach is not performing super well with the distilbert model, as it gives a relatively high alignment score even for non-correlated tasks. Example, try Sentiment analysis with Image classification.\n",
    "\n",
    "We are trying now another BERT model that is specifically trained to produce *semantically meaningful embeddings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ef4f82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5297174\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "topic_1 = 'Cars classification'\n",
    "topic_2 = 'Animals classification'\n",
    "\n",
    "mu_1 = sentence_model.encode(topic_1)\n",
    "mu_2 = sentence_model.encode(topic_2)\n",
    "\n",
    "# Normalizing the vectors\n",
    "mu_1 = mu_1 / np.linalg.norm(mu_1)\n",
    "mu_2 = mu_2 / np.linalg.norm(mu_2)\n",
    "\n",
    "# Getting the alignement of topic_2 with respect to topic_1\n",
    "beta = np.sum(mu_1 * mu_2)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c18d92",
   "metadata": {},
   "source": [
    "**Remark:** This model seems to be okey. There is a logic in its results as it preserves the *ordering* of the alignment scores $\\beta$: lower scores to less correlated tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
